INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=200, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': 'cpu'}
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=200, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': 'cpu'}
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=200, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=200, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:1
Train: 13.43809986114502 VAL: 1.2474197149276733
INFO:root:2
Train: 0.6340885162353516 VAL: 0.6703274846076965
INFO:root:3
Train: 0.49811843037605286 VAL: 0.6465309858322144
INFO:root:4
Train: 0.40560296177864075 VAL: 0.7848736643791199
INFO:root:5
Train: 0.6693263649940491 VAL: 0.394234836101532
INFO:root:6
Train: 0.4880819022655487 VAL: 0.650861382484436
INFO:root:7
Train: 0.39157870411872864 VAL: 0.5392419099807739
INFO:root:8
Train: 0.3493715524673462 VAL: 0.9291280508041382
INFO:root:9
Train: 0.33122751116752625 VAL: 0.36656612157821655
INFO:root:10
Train: 0.3077768087387085 VAL: 0.37227728962898254
INFO:root:11
Train: 0.3020491302013397 VAL: 0.5304175615310669
INFO:root:12
Train: 0.2804279029369354 VAL: 0.38597872853279114
INFO:root:13
Train: 0.29736655950546265 VAL: 0.38946792483329773
INFO:root:14
Train: 0.29665514826774597 VAL: 0.3825514614582062
INFO:root:15
Train: 0.29584401845932007 VAL: 0.40185317397117615
INFO:root:16
Train: 0.2951873540878296 VAL: 0.37392348051071167
INFO:root:17
Train: 0.2934474050998688 VAL: 0.39516568183898926
INFO:root:18
Train: 0.29729920625686646 VAL: 0.36480364203453064
INFO:root:19
Train: 0.2887272536754608 VAL: 0.32325783371925354
INFO:root:20
Train: 0.2878694534301758 VAL: 0.37578731775283813
INFO:root:21
Train: 0.282067209482193 VAL: 0.35894882678985596
INFO:root:22
Train: 0.2929314076900482 VAL: 0.3696548640727997
INFO:root:23
Train: 0.28055956959724426 VAL: 0.39183980226516724
INFO:root:24
Train: 0.2728268802165985 VAL: 0.3744317889213562
INFO:root:25
Train: 0.28394997119903564 VAL: 0.385442852973938
INFO:root:26
Train: 0.29046839475631714 VAL: 0.37756410241127014
INFO:root:27
Train: 0.2937523424625397 VAL: 0.3976154625415802
INFO:root:28
Train: 0.29451701045036316 VAL: 0.3921510577201843
INFO:root:29
Train: 0.2909138798713684 VAL: 0.3678092658519745
INFO:root:30
Train: 0.29391106963157654 VAL: 0.3311046361923218
INFO:root:31
Train: 0.2956257462501526 VAL: 0.32857391238212585
INFO:root:32
Train: 0.29958102107048035 VAL: 0.3308635950088501
INFO:root:33
Train: 0.2784043848514557 VAL: 0.3734791576862335
INFO:root:34
Train: 0.2857859134674072 VAL: 0.3775535225868225
INFO:root:35
Train: 0.2877423167228699 VAL: 0.3661864101886749
INFO:root:36
Train: 0.2853336036205292 VAL: 0.38187897205352783
INFO:root:37
Train: 0.2800517976284027 VAL: 0.3687535524368286
INFO:root:38
Train: 0.2886962890625 VAL: 0.40183117985725403
INFO:root:39
Train: 0.2847544550895691 VAL: 0.3701445162296295
INFO:root:40
Train: 0.28722742199897766 VAL: 0.3811276853084564
INFO:root:41
Train: 0.28303784132003784 VAL: 0.39556798338890076
INFO:root:42
Train: 0.2795657217502594 VAL: 0.3774629235267639
INFO:root:43
Train: 0.28683847188949585 VAL: 0.3698122203350067
INFO:root:44
Train: 0.2843984067440033 VAL: 0.3864355981349945
INFO:root:45
Train: 0.2877281904220581 VAL: 0.3974396288394928
INFO:root:46
Train: 0.2800549864768982 VAL: 0.3826903700828552
INFO:root:47
Train: 0.2723381817340851 VAL: 0.39406728744506836
INFO:root:48
Train: 0.28593119978904724 VAL: 0.39912065863609314
INFO:root:49
Train: 0.25812873244285583 VAL: 0.3977324664592743
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=200, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:1
Train: 17.9339656829834 VAL: 0.9824674129486084
INFO:root:2
Train: 0.7658610343933105 VAL: 0.6826847195625305
INFO:root:3
Train: 0.5765292048454285 VAL: 0.5093103051185608
INFO:root:4
Train: 0.4472329318523407 VAL: 0.5363372564315796
INFO:root:5
Train: 0.3687232434749603 VAL: 0.5644556283950806
INFO:root:6
Train: 0.376625657081604 VAL: 0.3782331645488739
INFO:root:7
Train: 0.3306320905685425 VAL: 0.6222571730613708
INFO:root:8
Train: 0.33134138584136963 VAL: 0.3841875195503235
INFO:root:9
Train: 0.31558436155319214 VAL: 0.33784037828445435
INFO:root:10
Train: 0.2915893495082855 VAL: 0.5782504677772522
INFO:root:11
Train: 0.31379976868629456 VAL: 0.39535295963287354
INFO:root:12
Train: 0.3007294535636902 VAL: 0.6055806875228882
INFO:root:13
Train: 0.3261605203151703 VAL: 0.3641703128814697
INFO:root:14
Train: 0.2943979501724243 VAL: 0.4003225862979889
INFO:root:15
Train: 0.29022151231765747 VAL: 0.3654438853263855
INFO:root:16
Train: 0.30405983328819275 VAL: 0.3211773633956909
INFO:root:17
Train: 0.2982151210308075 VAL: 0.32993924617767334
INFO:root:18
Train: 0.29488900303840637 VAL: 0.4129143953323364
INFO:root:19
Train: 0.28900769352912903 VAL: 0.311490535736084
INFO:root:20
Train: 0.2957753837108612 VAL: 0.31538084149360657
INFO:root:21
Train: 0.290484219789505 VAL: 0.3419950604438782
INFO:root:22
Train: 0.2984605133533478 VAL: 0.31834012269973755
INFO:root:23
Train: 0.29033711552619934 VAL: 0.3375343084335327
INFO:root:24
Train: 0.2841043174266815 VAL: 0.321670800447464
INFO:root:25
Train: 0.28330862522125244 VAL: 0.3387334644794464
INFO:root:26
Train: 0.2855609655380249 VAL: 0.3255431354045868
INFO:root:27
Train: 0.2925296127796173 VAL: 0.36739808320999146
INFO:root:28
Train: 0.2923889458179474 VAL: 0.3236095607280731
INFO:root:29
Train: 0.2777724266052246 VAL: 0.31362828612327576
INFO:root:30
Train: 0.2956357002258301 VAL: 0.37984082102775574
INFO:root:31
Train: 0.2868395745754242 VAL: 0.38439300656318665
INFO:root:32
Train: 0.27020081877708435 VAL: 0.39187225699424744
INFO:root:33
Train: 0.28797847032546997 VAL: 0.3625722825527191
INFO:root:34
Train: 0.2846106290817261 VAL: 0.35967063903808594
INFO:root:35
Train: 0.2835967242717743 VAL: 0.3679071068763733
INFO:root:36
Train: 0.28605327010154724 VAL: 0.37531569600105286
INFO:root:37
Train: 0.2732553780078888 VAL: 0.3831426501274109
INFO:root:38
Train: 0.2785751521587372 VAL: 0.39205679297447205
INFO:root:39
Train: 0.28631776571273804 VAL: 0.3931565582752228
INFO:root:40
Train: 0.2852444350719452 VAL: 0.3875492811203003
INFO:root:41
Train: 0.2917585074901581 VAL: 0.3794533908367157
INFO:root:42
Train: 0.2706230580806732 VAL: 0.39186611771583557
INFO:root:43
Train: 0.28581589460372925 VAL: 0.3884100019931793
INFO:root:44
Train: 0.28512561321258545 VAL: 0.3882189691066742
INFO:root:45
Train: 0.2768978178501129 VAL: 0.3986670970916748
INFO:root:46
Train: 0.2792356610298157 VAL: 0.38949158787727356
INFO:root:47
Train: 0.2840230166912079 VAL: 0.38502949476242065
INFO:root:48
Train: 0.26604339480400085 VAL: 0.3949747681617737
INFO:root:49
Train: 0.28695544600486755 VAL: 0.39863917231559753
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=200, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:1
Train: 19.678983688354492 VAL: 0.9330534934997559
INFO:root:2
Train: 0.659731388092041 VAL: 0.6805030703544617
INFO:root:3
Train: 0.5731543898582458 VAL: 0.5598790645599365
INFO:root:4
Train: 0.4558543860912323 VAL: 0.5181180834770203
INFO:root:5
Train: 0.40055909752845764 VAL: 0.544379472732544
INFO:root:6
Train: 0.41638925671577454 VAL: 0.6001952290534973
INFO:root:7
Train: 0.3664916753768921 VAL: 0.4909191429615021
INFO:root:8
Train: 0.3384568393230438 VAL: 0.7965136766433716
INFO:root:9
Train: 0.3499275743961334 VAL: 0.8198952674865723
INFO:root:10
Train: 0.3064451813697815 VAL: 0.3355807065963745
INFO:root:11
Train: 0.3178482949733734 VAL: 0.3409084975719452
INFO:root:12
Train: 0.3073710501194 VAL: 0.4718599021434784
INFO:root:13
Train: 0.30193302035331726 VAL: 0.3765203356742859
INFO:root:14
Train: 0.29874035716056824 VAL: 0.38242724537849426
INFO:root:15
Train: 0.2938680946826935 VAL: 0.3427436649799347
INFO:root:16
Train: 0.29718253016471863 VAL: 0.3240959644317627
INFO:root:17
Train: 0.3031407296657562 VAL: 0.3433414399623871
INFO:root:18
Train: 0.28254345059394836 VAL: 0.3426300883293152
INFO:root:19
Train: 0.2867501676082611 VAL: 0.34189674258232117
INFO:root:20
Train: 0.2618326246738434 VAL: 0.35082104802131653
INFO:root:21
Train: 0.29112717509269714 VAL: 0.358245313167572
INFO:root:22
Train: 0.2753896713256836 VAL: 0.35532575845718384
INFO:root:23
Train: 0.27339258790016174 VAL: 0.33966779708862305
INFO:root:24
Train: 0.2909580171108246 VAL: 0.3629271388053894
INFO:root:25
Train: 0.27200260758399963 VAL: 0.3781680166721344
INFO:root:26
Train: 0.2738244831562042 VAL: 0.3816116154193878
INFO:root:27
Train: 0.29511889815330505 VAL: 0.3804425001144409
INFO:root:28
Train: 0.2750505805015564 VAL: 0.3673509359359741
INFO:root:29
Train: 0.2775783836841583 VAL: 0.3552759289741516
INFO:root:30
Train: 0.28662118315696716 VAL: 0.35064420104026794
INFO:root:31
Train: 0.26281502842903137 VAL: 0.37797799706459045
INFO:root:32
Train: 0.28801190853118896 VAL: 0.3686428666114807
INFO:root:33
Train: 0.28437912464141846 VAL: 0.38050052523612976
INFO:root:34
Train: 0.2846839427947998 VAL: 0.3894696831703186
INFO:root:35
Train: 0.2831895351409912 VAL: 0.37683600187301636
INFO:root:36
Train: 0.2768815755844116 VAL: 0.38603049516677856
INFO:root:37
Train: 0.2848280370235443 VAL: 0.3764572739601135
INFO:root:38
Train: 0.2857300341129303 VAL: 0.35901057720184326
INFO:root:39
Train: 0.28880760073661804 VAL: 0.371566504240036
INFO:root:40
Train: 0.28354117274284363 VAL: 0.3123568892478943
INFO:root:41
Train: 0.2859230935573578 VAL: 0.3147084712982178
INFO:root:42
Train: 0.2816240191459656 VAL: 0.3091062009334564
INFO:root:43
Train: 0.2759716212749481 VAL: 0.3163338899612427
INFO:root:44
Train: 0.2792598307132721 VAL: 0.3065980076789856
INFO:root:45
Train: 0.28641456365585327 VAL: 0.3864627480506897
INFO:root:46
Train: 0.2860985994338989 VAL: 0.35365191102027893
INFO:root:47
Train: 0.2654896080493927 VAL: 0.36849015951156616
INFO:root:48
Train: 0.26663389801979065 VAL: 0.36683446168899536
INFO:root:49
Train: 0.28364264965057373 VAL: 0.3626314401626587
INFO:root:50
Train: 0.2777591347694397 VAL: 0.3645326495170593
INFO:root:51
Train: 0.2826618552207947 VAL: 0.38235658407211304
INFO:root:52
Train: 0.2816421389579773 VAL: 0.37465113401412964
INFO:root:53
Train: 0.2731606066226959 VAL: 0.36212533712387085
INFO:root:54
Train: 0.2756980359554291 VAL: 0.3661361336708069
INFO:root:55
Train: 0.2742244303226471 VAL: 0.3713161051273346
INFO:root:56
Train: 0.27743658423423767 VAL: 0.3688144385814667
INFO:root:57
Train: 0.28106290102005005 VAL: 0.36852407455444336
INFO:root:58
Train: 0.26414647698402405 VAL: 0.3703592121601105
INFO:root:59
Train: 0.2762316167354584 VAL: 0.37386125326156616
INFO:root:60
Train: 0.2845700979232788 VAL: 0.3748582601547241
INFO:root:61
Train: 0.2813204824924469 VAL: 0.3762223720550537
INFO:root:62
Train: 0.27844393253326416 VAL: 0.3717803657054901
INFO:root:63
Train: 0.2798252999782562 VAL: 0.37428411841392517
INFO:root:64
Train: 0.27587202191352844 VAL: 0.3761126399040222
INFO:root:65
Train: 0.2807280123233795 VAL: 0.37750744819641113
INFO:root:66
Train: 0.24008126556873322 VAL: 0.38016369938850403
INFO:root:67
Train: 0.2812233865261078 VAL: 0.37811967730522156
INFO:root:68
Train: 0.2685079574584961 VAL: 0.37806203961372375
INFO:root:69
Train: 0.27947506308555603 VAL: 0.3788231909275055
INFO:root:70
Train: 0.2786072790622711 VAL: 0.37780246138572693
INFO:root:71
Train: 0.25910985469818115 VAL: 0.3792358934879303
INFO:root:72
Train: 0.27830567955970764 VAL: 0.3801973760128021
INFO:root:73
Train: 0.2811742126941681 VAL: 0.3806062638759613
INFO:root:74
Train: 0.27223217487335205 VAL: 0.3829331696033478
INFO:root:BEST VAL: 0.3829331696033478 TEST : 0.3268445134162903
INFO:root:75
Train: 0.28424689173698425 VAL: 0.3075888156890869
INFO:root:BEST VAL: 0.3075888156890869 TEST : 0.3268445134162903
INFO:root:76
Train: 0.2860930860042572 VAL: 0.3491731882095337
INFO:root:BEST VAL: 0.3491731882095337 TEST : 0.3268445134162903
INFO:root:77
Train: 0.2786003649234772 VAL: 0.31317904591560364
INFO:root:BEST VAL: 0.31317904591560364 TEST : 0.3268445134162903
INFO:root:78
Train: 0.28086501359939575 VAL: 0.3098700940608978
INFO:root:BEST VAL: 0.3098700940608978 TEST : 0.3268445134162903
INFO:root:79
Train: 0.27718931436538696 VAL: 0.3130607306957245
INFO:root:BEST VAL: 0.3130607306957245 TEST : 0.3268445134162903
INFO:root:80
Train: 0.2791811525821686 VAL: 0.3084787130355835
INFO:root:BEST VAL: 0.3084787130355835 TEST : 0.3268445134162903
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=200, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:1
Train: 17.220237731933594 VAL: 0.8672518730163574
INFO:root:2
Train: 0.7058491706848145 VAL: 0.9243226647377014
INFO:root:3
Train: 0.6574448943138123 VAL: 0.544166088104248
INFO:root:4
Train: 0.5254402160644531 VAL: 0.49499088525772095
INFO:root:5
Train: 0.4149632155895233 VAL: 0.4632274806499481
INFO:root:6
Train: 0.4154694974422455 VAL: 0.5197353959083557
INFO:root:7
Train: 0.40633881092071533 VAL: 0.5646137595176697
INFO:root:8
Train: 0.40298140048980713 VAL: 0.5310420393943787
INFO:root:9
Train: 0.393545538187027 VAL: 0.365470826625824
INFO:root:10
Train: 0.35048821568489075 VAL: 0.386650949716568
INFO:root:11
Train: 0.33888542652130127 VAL: 0.5040200352668762
INFO:root:12
Train: 0.31527385115623474 VAL: 0.51291823387146
INFO:root:13
Train: 0.31821736693382263 VAL: 0.3704315423965454
INFO:root:14
Train: 0.30857083201408386 VAL: 0.3379196524620056
INFO:root:15
Train: 0.3274534046649933 VAL: 0.4225527346134186
INFO:root:16
Train: 0.3012149930000305 VAL: 0.605157732963562
INFO:root:17
Train: 0.29521504044532776 VAL: 0.4098576605319977
INFO:root:18
Train: 0.3096879720687866 VAL: 0.3816392421722412
INFO:root:19
Train: 0.29614219069480896 VAL: 0.35918569564819336
INFO:root:20
Train: 0.2801133394241333 VAL: 0.3151179254055023
INFO:root:21
Train: 0.2772170603275299 VAL: 0.33099618554115295
INFO:root:22
Train: 0.28397858142852783 VAL: 0.31027576327323914
INFO:root:23
Train: 0.29212573170661926 VAL: 0.33902207016944885
INFO:root:24
Train: 0.2977944612503052 VAL: 0.31241706013679504
INFO:root:25
Train: 0.29211193323135376 VAL: 0.320680171251297
INFO:root:26
Train: 0.2804747521877289 VAL: 0.322990357875824
INFO:root:27
Train: 0.29339978098869324 VAL: 0.31646373867988586
INFO:root:28
Train: 0.29178735613822937 VAL: 0.3246212601661682
INFO:root:29
Train: 0.28163644671440125 VAL: 0.3182297646999359
INFO:root:30
Train: 0.29072847962379456 VAL: 0.32239049673080444
INFO:root:31
Train: 0.2915624976158142 VAL: 0.3226330578327179
INFO:root:32
Train: 0.2909838557243347 VAL: 0.349378377199173
INFO:root:33
Train: 0.29774489998817444 VAL: 0.3352516293525696
INFO:root:34
Train: 0.2868111729621887 VAL: 0.3434427082538605
INFO:root:35
Train: 0.27893853187561035 VAL: 0.3617618978023529
INFO:root:36
Train: 0.2854067087173462 VAL: 0.3806663751602173
INFO:root:37
Train: 0.2859705686569214 VAL: 0.37610113620758057
INFO:root:38
Train: 0.2932935953140259 VAL: 0.3704386055469513
INFO:root:39
Train: 0.2623879909515381 VAL: 0.3640333116054535
INFO:root:40
Train: 0.288356751203537 VAL: 0.37217724323272705
INFO:root:41
Train: 0.2730178236961365 VAL: 0.3677366077899933
INFO:root:42
Train: 0.2895718216896057 VAL: 0.3740783631801605
INFO:root:43
Train: 0.2877022325992584 VAL: 0.36308616399765015
INFO:root:44
Train: 0.2781996726989746 VAL: 0.3729645311832428
INFO:root:45
Train: 0.2790185511112213 VAL: 0.38310688734054565
INFO:root:46
Train: 0.28257986903190613 VAL: 0.3809604048728943
INFO:root:47
Train: 0.28194355964660645 VAL: 0.38084131479263306
INFO:root:48
Train: 0.27610448002815247 VAL: 0.3610835671424866
INFO:root:49
Train: 0.2794559597969055 VAL: 0.35925352573394775
INFO:root:50
Train: 0.2606353759765625 VAL: 0.380840003490448
INFO:root:51
Train: 0.28575190901756287 VAL: 0.3871105909347534
INFO:root:52
Train: 0.2864515483379364 VAL: 0.3827631175518036
INFO:root:BEST VAL: 0.3827631175518036 TEST : 0.23488891124725342
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=200, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:1
Train: 14.268523216247559 VAL: 1.065020203590393
INFO:root:2
Train: 0.7602860331535339 VAL: 0.6947091817855835
INFO:root:3
Train: 0.5918015837669373 VAL: 0.5477558970451355
INFO:root:4
Train: 0.5055165886878967 VAL: 0.44014859199523926
INFO:root:5
Train: 0.37256181240081787 VAL: 0.3762073218822479
INFO:root:6
Train: 0.3597605228424072 VAL: 0.5065542459487915
INFO:root:7
Train: 0.3252796232700348 VAL: 0.5526754856109619
INFO:root:8
Train: 0.32296571135520935 VAL: 0.3334519565105438
INFO:root:9
Train: 0.3134688436985016 VAL: 0.3193010687828064
INFO:root:10
Train: 0.2929480969905853 VAL: 0.3285106122493744
INFO:root:11
Train: 0.2952563464641571 VAL: 0.33087608218193054
INFO:root:12
Train: 0.2917245924472809 VAL: 0.33276811242103577
INFO:root:13
Train: 0.295157253742218 VAL: 0.31534093618392944
INFO:root:14
Train: 0.29850319027900696 VAL: 0.3533383309841156
INFO:root:15
Train: 0.2762121260166168 VAL: 0.3697906732559204
INFO:root:16
Train: 0.2906845808029175 VAL: 0.35705626010894775
INFO:root:17
Train: 0.28613945841789246 VAL: 0.3638318181037903
INFO:root:18
Train: 0.28382137417793274 VAL: 0.3640131652355194
INFO:root:19
Train: 0.2877310812473297 VAL: 0.395383358001709
INFO:root:20
Train: 0.3131399154663086 VAL: 0.4160861372947693
INFO:root:21
Train: 0.3355928957462311 VAL: 0.4497012495994568
INFO:root:22
Train: 0.33548250794410706 VAL: 0.3781684339046478
INFO:root:23
Train: 0.32753461599349976 VAL: 0.3298293352127075
INFO:root:24
Train: 0.316762775182724 VAL: 0.3417462110519409
INFO:root:25
Train: 0.2820979058742523 VAL: 0.3393966555595398
INFO:root:26
Train: 0.29829296469688416 VAL: 0.362493634223938
INFO:root:27
Train: 0.2834251821041107 VAL: 0.36686742305755615
INFO:root:28
Train: 0.28181713819503784 VAL: 0.38839319348335266
INFO:root:29
Train: 0.29256314039230347 VAL: 0.33468449115753174
INFO:root:30
Train: 0.29069146513938904 VAL: 0.35438740253448486
INFO:root:31
Train: 0.29427167773246765 VAL: 0.3413480818271637
INFO:root:32
Train: 0.2906842529773712 VAL: 0.3431551456451416
INFO:root:33
Train: 0.29587849974632263 VAL: 0.34797951579093933
INFO:root:34
Train: 0.29361340403556824 VAL: 0.3271881937980652
INFO:root:35
Train: 0.28879719972610474 VAL: 0.3464246690273285
INFO:root:36
Train: 0.2810218334197998 VAL: 0.3352788984775543
INFO:root:37
Train: 0.28880688548088074 VAL: 0.3358936011791229
INFO:root:38
Train: 0.28825053572654724 VAL: 0.3336867094039917
INFO:root:39
Train: 0.2804996967315674 VAL: 0.3238722085952759
INFO:root:40
Train: 0.28640204668045044 VAL: 0.32298189401626587
INFO:root:41
Train: 0.2767309844493866 VAL: 0.3277926743030548
INFO:root:42
Train: 0.28398895263671875 VAL: 0.3274611532688141
INFO:root:43
Train: 0.27726873755455017 VAL: 0.3235762417316437
INFO:root:BEST VAL: 0.3235762417316437 TEST : 0.29740914702415466
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=1, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:1
Train: 18.85370635986328 VAL: 1.403510570526123
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=1, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:1
Train: 18.879756927490234 VAL: 0.9010175466537476
INFO:root:BEST VAL: 0.9010175466537476 TEST : 0.3087451457977295
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=1, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=1, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=1, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=1, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=1, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:1
Train: 9.576200485229492 VAL: 0.7744582891464233
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=1, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:1
Train: 21.605791091918945 VAL: 1.0557111501693726
INFO:root:BEST VAL: 1.0557111501693726 TEST : 0.3095504343509674
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=1, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:1
Train: 14.25771713256836 VAL: 0.9854204058647156
INFO:root:BEST VAL: 0.9854204058647156 TEST : 0.3095504343509674
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=200, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:1
Train: 19.260086059570312 VAL: 1.317423701286316
INFO:root:2
Train: 0.8306017518043518 VAL: 0.7827973365783691
INFO:root:3
Train: 0.6055380702018738 VAL: 0.8108564019203186
INFO:root:4
Train: 0.5467972159385681 VAL: 0.5009375214576721
INFO:root:5
Train: 0.3947766125202179 VAL: 0.45013707876205444
INFO:root:6
Train: 0.3804311752319336 VAL: 0.656098484992981
INFO:root:7
Train: 0.36069175601005554 VAL: 0.47707581520080566
INFO:root:8
Train: 0.4963385760784149 VAL: 0.409457802772522
INFO:root:9
Train: 0.39429688453674316 VAL: 0.4702099561691284
INFO:root:10
Train: 0.3986496925354004 VAL: 0.4012828469276428
INFO:root:11
Train: 0.3372783362865448 VAL: 0.38649630546569824
INFO:root:12
Train: 0.32970568537712097 VAL: 0.32904520630836487
INFO:root:13
Train: 0.31476980447769165 VAL: 0.3294331431388855
INFO:root:14
Train: 0.31998807191848755 VAL: 0.3177996575832367
INFO:root:15
Train: 0.3164435923099518 VAL: 0.34918373823165894
INFO:root:16
Train: 0.3112732768058777 VAL: 0.3145943284034729
INFO:root:17
Train: 0.31296083331108093 VAL: 0.35309404134750366
INFO:root:18
Train: 0.3179573118686676 VAL: 0.3362533450126648
INFO:root:19
Train: 0.3026920258998871 VAL: 0.33122044801712036
INFO:root:20
Train: 0.3107414245605469 VAL: 0.3154110610485077
INFO:root:21
Train: 0.30824610590934753 VAL: 0.3635423183441162
INFO:root:22
Train: 0.31122738122940063 VAL: 0.33881115913391113
INFO:root:23
Train: 0.3100843131542206 VAL: 0.3180224299430847
INFO:root:24
Train: 0.28631460666656494 VAL: 0.3139985203742981
INFO:root:25
Train: 0.2994248569011688 VAL: 0.3371046185493469
INFO:root:26
Train: 0.3032871186733246 VAL: 0.33524173498153687
INFO:root:27
Train: 0.3084743320941925 VAL: 0.3174712657928467
INFO:root:28
Train: 0.3036753237247467 VAL: 0.3197218179702759
INFO:root:29
Train: 0.30513668060302734 VAL: 0.3469541072845459
INFO:root:30
Train: 0.3060567378997803 VAL: 0.3328428268432617
INFO:root:31
Train: 0.3028131425380707 VAL: 0.32399022579193115
INFO:root:32
Train: 0.30694547295570374 VAL: 0.34132570028305054
INFO:root:33
Train: 0.30422866344451904 VAL: 0.3535236120223999
INFO:root:34
Train: 0.30493006110191345 VAL: 0.33666589856147766
INFO:root:35
Train: 0.3040236234664917 VAL: 0.33032602071762085
INFO:root:36
Train: 0.29056262969970703 VAL: 0.3465577960014343
INFO:root:37
Train: 0.30066752433776855 VAL: 0.3428758382797241
INFO:root:38
Train: 0.29388055205345154 VAL: 0.336561918258667
INFO:root:39
Train: 0.299979567527771 VAL: 0.34193384647369385
INFO:root:40
Train: 0.30033767223358154 VAL: 0.3394944667816162
INFO:root:41
Train: 0.30550625920295715 VAL: 0.33576470613479614
INFO:root:42
Train: 0.30701008439064026 VAL: 0.3509640097618103
INFO:root:43
Train: 0.2998833954334259 VAL: 0.34367817640304565
INFO:root:44
Train: 0.30307039618492126 VAL: 0.336647629737854
INFO:root:45
Train: 0.28111860156059265 VAL: 0.336340069770813
INFO:root:46
Train: 0.29716306924819946 VAL: 0.3498331606388092
INFO:root:47
Train: 0.2922237813472748 VAL: 0.34659260511398315
INFO:root:48
Train: 0.29096004366874695 VAL: 0.3400019109249115
INFO:root:49
Train: 0.2737579047679901 VAL: 0.33827829360961914
INFO:root:50
Train: 0.2996165454387665 VAL: 0.34167003631591797
INFO:root:51
Train: 0.3013679087162018 VAL: 0.3382883071899414
INFO:root:52
Train: 0.2836812138557434 VAL: 0.3394588232040405
INFO:root:53
Train: 0.2731076180934906 VAL: 0.33879679441452026
INFO:root:54
Train: 0.29015061259269714 VAL: 0.3394780158996582
INFO:root:BEST VAL: 0.3394780158996582 TEST : 0.27970486879348755
INFO:root:Namespace(quiet=False, run_id=None, config=None, epochs=200, fold=0, batch_size=128, learn_rate=0.001, betas=(0.9, 0.999), weight_decay=0.001, hidden_size=32, kernel_init='skew-symmetric', note='', seed=None, nlayers=4, attn_head=2, latent_dim=256, dataset='ushcn')
INFO:root:{'input_dim': 5, 'attn_head': 2, 'latent_dim': 256, 'n_layers': 4, 'device': device(type='cuda')}
INFO:root:1
Train: 15.19197940826416 VAL: 0.6724036931991577
INFO:root:2
Train: 0.6523851156234741 VAL: 0.7519130110740662
INFO:root:3
Train: 0.6143879294395447 VAL: 0.5013667345046997
INFO:root:4
Train: 0.48551490902900696 VAL: 0.3961426317691803
INFO:root:5
Train: 0.414133757352829 VAL: 1.017959713935852
INFO:root:6
Train: 0.9062222838401794 VAL: 0.5937552452087402
INFO:root:7
Train: 0.635392427444458 VAL: 0.45149022340774536
INFO:root:8
Train: 0.41152772307395935 VAL: 0.4050746262073517
INFO:root:9
Train: 0.35571375489234924 VAL: 1.24820077419281
INFO:root:10
Train: 0.3582817614078522 VAL: 0.42423292994499207
INFO:root:11
Train: 0.3465169370174408 VAL: 0.44081559777259827
INFO:root:12
Train: 0.35441645979881287 VAL: 0.4283277094364166
INFO:root:13
Train: 0.356964111328125 VAL: 0.4162163734436035
INFO:root:14
Train: 0.33342739939689636 VAL: 0.5315743684768677
INFO:root:15
Train: 0.30914804339408875 VAL: 0.3930816054344177
INFO:root:16
Train: 0.32682058215141296 VAL: 0.40148860216140747
INFO:root:17
Train: 0.33706533908843994 VAL: 0.35486626625061035
INFO:root:18
Train: 0.32578417658805847 VAL: 0.3596220016479492
INFO:root:19
Train: 0.28976425528526306 VAL: 0.32162341475486755
INFO:root:20
Train: 0.30161193013191223 VAL: 0.3113490045070648
INFO:root:21
Train: 0.3070979416370392 VAL: 0.31352704763412476
INFO:root:22
Train: 0.3048405945301056 VAL: 0.30909812450408936
INFO:root:23
Train: 0.3076438307762146 VAL: 0.31928402185440063
INFO:root:24
Train: 0.3151869773864746 VAL: 0.31536027789115906
INFO:root:25
Train: 0.30781158804893494 VAL: 0.31420275568962097
INFO:root:26
Train: 0.3111504912376404 VAL: 0.3183477222919464
INFO:root:27
Train: 0.3072356581687927 VAL: 0.3200730085372925
INFO:root:28
Train: 0.30696818232536316 VAL: 0.32830309867858887
INFO:root:29
Train: 0.28456974029541016 VAL: 0.3179384469985962
INFO:root:30
Train: 0.2952539026737213 VAL: 0.3174893260002136
INFO:root:31
Train: 0.3021136224269867 VAL: 0.3270800709724426
INFO:root:32
Train: 0.3045189082622528 VAL: 0.31736767292022705
INFO:root:33
Train: 0.29893872141838074 VAL: 0.3195881247520447
INFO:root:34
Train: 0.30270278453826904 VAL: 0.31842511892318726
INFO:root:35
Train: 0.3006478250026703 VAL: 0.3231627941131592
INFO:root:36
Train: 0.2975776493549347 VAL: 0.333426296710968
INFO:root:37
Train: 0.29577016830444336 VAL: 0.3280351758003235
INFO:root:38
Train: 0.2759546935558319 VAL: 0.3266613781452179
INFO:root:39
Train: 0.2779659032821655 VAL: 0.32603132724761963
INFO:root:40
Train: 0.30251702666282654 VAL: 0.3286215662956238
INFO:root:41
Train: 0.27669569849967957 VAL: 0.33054929971694946
INFO:root:42
Train: 0.2832653224468231 VAL: 0.3327108919620514
INFO:root:43
Train: 0.30018189549446106 VAL: 0.3277689814567566
INFO:root:44
Train: 0.29508742690086365 VAL: 0.3322986960411072
INFO:root:45
Train: 0.2978748381137848 VAL: 0.33112600445747375
INFO:root:46
Train: 0.29758337140083313 VAL: 0.33175796270370483
INFO:root:47
Train: 0.29860666394233704 VAL: 0.3315200209617615
INFO:root:48
Train: 0.2998414933681488 VAL: 0.3317548632621765
INFO:root:49
Train: 0.2968878746032715 VAL: 0.3299006223678589
INFO:root:50
Train: 0.2969261109828949 VAL: 0.3328135013580322
INFO:root:51
Train: 0.2734861671924591 VAL: 0.3336731195449829
INFO:root:52
Train: 0.3001793920993805 VAL: 0.3367244005203247
INFO:root:BEST VAL: 0.3367244005203247 TEST : 0.2822304368019104
